\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

\section{Desarrollo del Proyecto}

El desarrollo de este proyecto se ha llevado a cabo utilizando una metodología ágil basada en Scrum. Este enfoque proporciona flexibilidad necesaria para adaptar cambios en los requerimientos y para mejorar iterativamente el proyecto durante su desarrollo.
Las metodología ágiles se basan en la idea de dividir el proyecto en ciclos iterativos llamados "sprints", donde se llevan a cabo actividades de planificación, desarrollo, pruebas y revisión. 
Estos sprints suelen tener una duración fija, en este caso han sido de 1 semana con alguna excepción justificada.

\subsection{Metodología Scrum}
En el documento \textit{Anexos} se explica en detalle el proceso de desarrollo del proyecto.

En resumen el desarrollo ha consistido en los siguientes pasos iterativos:

\begin{enumerate}
\item \textbf{Planificación inicial}: Etapa de definición de objetivos generales del proyecto. Creación del Product Backlog.

\item \textbf{Reuniones de Sprint}: Al comienzo de cada sprint, se realiza una pequeña reunión para revisar las tareas realizadas y seleccionar las tareas a desarrollar.

\item \textbf{Desarrollo del sprint}: Durante el sprint, se trabaja en cada una de las tareas asignadas.

\item \textbf{Mejora continua}: La metodología ágil promueve la mejora continua y el aprendizaje a lo largo del desarrollo del proyecto.
\end{enumerate}

\subsection{Análisis}

El objetivo del proyecto es aplicar técnicas de Inteligencia Artificial (IA) y Aprendizaje Automático (ML) para la clasificación de estilos musicales. 
Durante la fase de análisis, se estudiaron varios modelos de ML, eligiendo finalmente la Red Neuronal Convolucional (CNN) debido a su mayor eficacia.

Otro factor a tener en cuenta en un proyecto de IA es el conjunto de datos. Se han planteado diversos conjuntos de datos para entrenar el modelo como:
\begin{itemize}

\item \textbf{GTZAN Dataset}: conjunto de datos muy utilizado en la clasificación musical. Recopilado por George Tzanetakis en 2002, consta de \textbf{1000 fragmentos} de audio de \textbf{30 segundos}, distribuidos en \textbf{10 géneros musicales}.

\item \textbf{Million Song Dataset}: conjunto de datos formado por las \textit{características musicales y metadatos} de un millón de canciones populares. No incluye pistas de audio.

\item \textbf{MagnaTagATune}: conjunto de datos formado por \textbf{25863 fragmentos} de audio de \textbf{29 segundos}.

\item \textbf{FMA (Free Music Archive)}: conjunto de datos musicales, libre de derechos, constituido por \textbf{106574 fragmentos} de audio de \textbf{30 segundos}. 

\end{itemize}

Finalmente se ha elegido el conjunto de datos \textbf{FMA (Free Music Archive)} para realizar el entrenamiento del modelo. Esto es debido a que ofrece una gran cantidad de pistas musicales libres de derechos. Además, al incluir las pistas musicales (a diferencia de Million Song Dataset por ejemplo)
es posible extraer las características de forma manual utilizando librosa u otras herramientas.

\subsection{Diseño}

En cuanto al diseño de la aplicación, en un primer momento se pensó en el desarrollo de una aplicación de escritorio. Sin embargo, esta idea fue descartada y se optó por desarrollar una aplicación web debido a las siguientes razones:

\begin{itemize}

\item \textbf{Evolución tecnológica}: Es una realidad que las aplicaciones web están dominando el momento tecnológico actual, por lo que realizar el proyecto en web es una buena manera de estar al día de esta tecnología.

\item \textbf{Accesibilidad}: La aplicación web es accesible desde cualquier dispositivo con una conexión a internet, independientemente del sistema operativo. De esta manera se amplía el alcance de la aplicación.

\item \textbf{Actualizaciones}: Cuando se actualiza una aplicación web, automaticamente los usuarios reciben la última versión disponible en producción, eliminando la necesidad de descargar e instalar actualizaciones manualmente. 

\item \textbf{Mantenimiento}: Generalmente es más sencillo mantener una aplicación web que una aplicación de escritorio, ya que factores como hardware o sistema operativo del usuario desaparecen.
\end{itemize}

El marco de trabajo escogido para realizar la implementación de la aplicación web ha sido:

\begin{itemize}

\item \textbf{Backend}: Para el backend se ha utilizado Flask, un framework web comptabile con Python, pensado para realizar aplicaciones web y APIs.

\item \textbf{Frontend}: Para el frontend, se ha optado por HTML y CSS. HTML es un lenguaje de marcado que define la estructura y contenidos de las páginas web que conforman la aplicación. CSS es utilizado para definir los estilos de los documentos HTML y añadir elementos atractivos para el usuario.

\end{itemize}

\newpage

\section{Extracción de características de audio}

En este sección se va a analizar el proceso de extracción de características de audio.

\subsection{Extracción de MFCC usando Python y librosa}

El método de extracción de MFCC está diseñado para recorrer todos los archivos en un directorio especificado y extraer las características MFCC de cada archivo de audio que encuentre. La explicación de los detalles relevantes es la siguiente:

\begin{enumerate}
\tightlist
\item \textbf{Carga de archivos}: Carga del archivo de audio, devolviendo tanto la señal de audio (y) como la frecuencia de muestreo (sr).

\item \textbf{Duración estándar}: Se extraen los primeros 25 segundos de cada pista de audio para obtener la misma dimensionalidad en todo el conjunto de datos.

\item\textbf{Extracción de coeficientes MFCC}: Se extraen 10 coeficientes MFCC de cada pista de audio. Se ha decidido extraer 10 coeficientes ya que es un número válido para realizar una posterior clasificación y su dimensionalidad es correcta para hacer el entrenamiento eficiente.

\item \textbf{Normalización}: Los coeficientes MFCC extraídos se normalizan restando la media y dividiendo por la desviación estándar. Este proceso se llama \textit{Normalización Z-Score} y se calcula de la siguiente manera.

\begin{equation}
z = \frac{x - \mu}{\sigma}
\end{equation}

donde \(z\) es el valor estandarizado, \(x\) es el valor de los coeficientes MFCC, \(\mu\) es la media de los coeficientes MFCC y \(\sigma\) es la desviación estándar de los coeficientes MFCC.

\end{enumerate}

\subsection{Extracción del resto de características}

Para generar visualizaciones, se ha repetido el proceso de extracción de características con:

\begin{enumerate}
\tightlist
\item \textbf{Onda de audio}

\item \textbf{Espectrogramas}

\item \textbf{Cromagramas}
\end{enumerate}

\newpage

\section{Técnicas de aumento de datos}

Las técnicas de aumentación de datos se basan en la generación de datos artificiales por medio de modificaciones de los datos originales. Esta técnica es importante en conjuntos de datos con pocos ejemplos o cuyos ejemplos no son suficientes para realizar un entrenamiento de calidad.
En este proyecto se han planteado diversas técnicas de aumentación de datos como por ejemplo:

\begin{itemize}
\tightlist

\item \textbf{Adicción de ruido de fondo}: la adicción de ruido de fondo es una técnica utilizada en los clips de audio que consiste en añadir un sonido estático, como ruido blanco o ruido ambiental, de esta manera se puede ayudar al modelo a entender los diferentes sonidos de fondo.

\item \textbf{Cambio de tono (pitch shifting)}: esta técnica consiste en variar la tonalidad o la frecuencia de la pista de audio sin cambiar su velocidad, de esta manera el modelo es capaz de generalizar mejor en situaciones donde hay diferentes tonalidades.

\item \textbf{Estiramiento temporal (time stretching)}: esta técnica, a diferencia del \textit{pitch shifting}, consiste en variar la velocidad del audio sin cambiar su tono o frecuencia. Esto ayuda al modelo a entender diferentes velocidades en el habla o en los instrumentos.

\item \textbf{Efectos auditivos}: se añaden efectos a la pista de audio como pueden ser eco, reverberación o chorus. De esta manera el modelo aprende a ser más robusto en estas situaciones.

\end{itemize}

\newpage

\section{Modelo de aprendizaje automático}

En esta sección se va a explicar en detalle las diferentes características del modelo de aprendizaje automático que se ha utilizado en el proyecto.

El modelo de aprendizaje automático utilizado es una representación de datos musicales procesados y algoritmos utilizados para realizar tareas de predicción de una clase.

\subsection{Datos}

Los datos con los que se alimentan al modelo utilizado son un conjunto de características musicales, en concreto 10 coeficientes MFCC, junto a su etiqueta correspondiente, el género musical. Con esta información el modelo aplicará algoritmos de machine learning para aprender de los datos y devolver nuevas predicciones
(generalizar) sobre datos nuevos o externos.

Los datos proceden de una gran cantidad de ficheros de audio, los cuales han sido procesados para extraer sus características más importantes. Una vez realizado este proceso, los datos procesados se guardan en un archivo, el cual será encargado de alimentar el modelo de aprendizaje automático.

Existen diversos problemas en este planteamiento, lo primero es la cantidad de datos a procesar. Los datos deben ser almacenados en alguna parte mientras se introducen al modelo. Esto puede suponer un problema si se cuenta con un equipo no demasiado potente que no puede almacenar todos los datos en la memoria.
Para solucionar este problema se puede recurrir al uso de datasets incrementales mediante generadores. De esta manera los datos se devuelven iterativamente y no es necesario que se almacenen todos juntos en memoria.

\subsection{Redes neuronales}

\section{Evaluación del rendimiento del modelo}

\subsection{Entrenamiento}

\subsection{Explotación}